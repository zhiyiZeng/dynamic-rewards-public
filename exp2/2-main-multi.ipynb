{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import multiprocessing\n",
    "from importlib import reload\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import distinctipy\n",
    "\n",
    "from DataLoader.DataLoader import DataLoader\n",
    "from DataLoader.DataBasedAgent import DataBasedAgent\n",
    "from DataLoader.DataRLAgent import DataRLAgent\n",
    "import DeepRLAgent.VanillaInput.Train as Train\n",
    "from PatternDetectionInCandleStick.Evaluation import Evaluation\n",
    "import shutil\n",
    "\n",
    "from utils import add_train_portfo, add_test_portfo, plot_return, calc_return, plot_action_point, calc_bh, setup_logger\n",
    "\n",
    "Train = reload(Train)\n",
    "DeepRL = Train.Train\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = \"cpu\"\n",
    "CURRENT_PATH = os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train(\n",
    "    DATASET_NAME, \n",
    "    split_point='2018-01-01', \n",
    "    begin_date='2010-01-01', \n",
    "    end_date='2020-08-24', \n",
    "    initial_investment=1000,\n",
    "    transaction_cost=0.0001,\n",
    "    load_from_file=True,\n",
    "    reward_type=\"profit\",\n",
    "    seed=42, \n",
    "    state_mode=1,\n",
    "    n_episodes=5,\n",
    "    lamb=0.0001,\n",
    "    GAMMA=0.7, \n",
    "    n_step=5, \n",
    "    BATCH_SIZE=10, \n",
    "    ReplayMemorySize=20,\n",
    "    TARGET_UPDATE=5,\n",
    "    window_size=None, \n",
    "    train_portfolios={},\n",
    "    test_portfolios={},\n",
    "    arms={},\n",
    "    show_all = False,\n",
    "    ratio_threshold=0.9,\n",
    "):\n",
    "    data_loader = DataLoader(DATASET_NAME, split_point=split_point, begin_date=begin_date, end_date=end_date, load_from_file=load_from_file)\n",
    "    \n",
    "    dataTrain_agent = DataRLAgent(data_loader.data_train, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "    dataTest_agent = DataRLAgent(data_loader.data_test, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    agent = DeepRL(data_loader, dataTrain_agent, dataTest_agent, \n",
    "                DATASET_NAME,  state_mode, window_size, transaction_cost,\n",
    "                BATCH_SIZE=BATCH_SIZE, GAMMA=GAMMA, ReplayMemorySize=ReplayMemorySize,\n",
    "                TARGET_UPDATE=TARGET_UPDATE, n_step=n_step, arms=arms)\n",
    "\n",
    "    agent.train(arms, n_episodes, ratio_threshold, seed, begin_date, end_date)\n",
    "\n",
    "    agent_eval = agent.test(initial_investment=initial_investment, test_type='train', model_dir=\"\")\n",
    "    train_portfolio = agent_eval.get_daily_portfolio_value()\n",
    "    \n",
    "    agent_test = agent.test(initial_investment=initial_investment, test_type='test', model_dir=\"\")\n",
    "    test_portfolio = agent_test.get_daily_portfolio_value()\n",
    "\n",
    "    arm = arms[0]\n",
    "    final_model_name = f\"{arm['name']}_{arm['lamb']}_{seed}\"\n",
    "\n",
    "    add_train_portfo(train_portfolios, final_model_name, train_portfolio)\n",
    "    add_test_portfo(test_portfolios, final_model_name, test_portfolio)\n",
    "\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_arm(arms, name, lamb):\n",
    "    arm = { \"index\": len(arms), \"name\": name, \"lamb\": lamb, \"sharpe_list\": [], \"cumreturn_list\": []}\n",
    "    arms.append(arm)\n",
    "\n",
    "\n",
    "def check_log(path, epochs):\n",
    "    # TODO 这里要根据情况改一下\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"r\", encoding=\"utf8\") as f:\n",
    "            content = f.read()\n",
    "            _epochs = re.findall(\"epoch: (\\d+),\", content)\n",
    "            if len(_epochs) > 0 and int(_epochs[-1]) == epochs - 1:\n",
    "                return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "\n",
    "def run(_file, begin_date, end_date, split_point, initial_investment, seed, epochs):\n",
    "    train_portfolios = {}\n",
    "    test_portfolios = {}\n",
    "\n",
    "    kwargs = {\n",
    "        \"begin_date\": begin_date, \n",
    "        \"end_date\": end_date, \n",
    "        \"split_point\": split_point, \n",
    "        \"load_from_file\": True, \n",
    "        \"transaction_cost\": 0.0000,\n",
    "        \"initial_investment\": initial_investment,\n",
    "        \"state_mode\": 1,\n",
    "        \"seed\": 42, \n",
    "        \"GAMMA\": 0.7, \n",
    "        \"n_step\": 5, \n",
    "        \"BATCH_SIZE\": 10, \n",
    "        \"ReplayMemorySize\": 20,\n",
    "        \"TARGET_UPDATE\": 5,\n",
    "        \"window_size\": None, \n",
    "        \"train_portfolios\": train_portfolios,\n",
    "        \"test_portfolios\": test_portfolios,\n",
    "        \"lamb\": 0.0,\n",
    "    }\n",
    "\n",
    "    arms = []\n",
    "    add_arm(arms, \"profit\", 0)\n",
    "    add_arm(arms, \"sharpe\", 0.01)\n",
    "    add_arm(arms, \"volatility\", 10)\n",
    "    add_arm(arms, \"regularized\", 0.01)\n",
    "    add_arm(arms, \"regularized\", 0.05)\n",
    "    add_arm(arms, \"regularized\", 0.1)\n",
    "    add_arm(arms, \"regularized\", 0.2)\n",
    "\n",
    "    for arm in arms:\n",
    "        print(_file, seed, begin_date, end_date, arm)\n",
    "\n",
    "        path = f\"./Results/{_file}/{begin_date}~{end_date}/{seed}/train_log/{arm['name']}-{arm['lamb']}.log\"\n",
    "        final_path = f\"./Results/{_file}/{begin_date}~{end_date}/{seed}/train_log/{seed}.log\"\n",
    "\n",
    "        is_finished = check_log(path, epochs)\n",
    "        if is_finished and os.path.exists(final_path):\n",
    "            print(f\"{arm['name']}已经跑完了，跳过...\")\n",
    "            continue\n",
    "        \n",
    "        kwargs.update({\n",
    "            \"DATASET_NAME\": _file,\n",
    "            \"reward_type\": \"\",\n",
    "            \"seed\": seed,\n",
    "            \"n_episodes\": epochs,\n",
    "            \"arms\": [arm],\n",
    "            \"show_all\": True,\n",
    "            \"ratio_threshold\": 3,\n",
    "            \"train_portfolios\": train_portfolios,\n",
    "            \"test_portfolios\": test_portfolios,\n",
    "        })\n",
    "    \n",
    "        data_loader = train(**kwargs)\n",
    "    \n",
    "    indexes = calc_return(data_loader, train_portfolios, test_portfolios)\n",
    "    \n",
    "    final_model_name = indexes.T[\"sharpe_train\"].sort_values(ascending=False).index[0]\n",
    "    src = f\"./Results/{_file}/{begin_date}~{end_date}/{seed}/train/model_{final_model_name}.pkl\"\n",
    "    dst = f\"./Results/{_file}/{begin_date}~{end_date}/{seed}/train/model_{seed}.pkl\"\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "    path = f\"./Results/{_file}/{begin_date}~{end_date}/{seed}/train_log/\"\n",
    "    logger, handler = setup_logger(f'{_file}-{seed}-final', f'{path}/{seed}.log')\n",
    "    logger.info(f\"symbol: {_file}, seed: {seed}, final reward type: {final_model_name}\")\n",
    "    logger.info(f\"symbol: {_file}, seed: {seed}, top 3: {indexes.T['sharpe_train'].sort_values(ascending=False).values[:3]}\")\n",
    "    logger.removeHandler(handler)\n",
    "    return seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_main(queue):\n",
    "    print(\"启动进程\")\n",
    "    while True:\n",
    "        try:\n",
    "            item = queue.get(block=True) #block=True means make a blocking call to wait for items in queue\n",
    "            if item is None:\n",
    "                print(\"运行结束...\")\n",
    "                break\n",
    "            seed = run(*item)\n",
    "            print(f\"seed: {seed}已经运行完毕...\")\n",
    "            time.sleep(2) \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"keyboard interrupt\")\n",
    "            import sys\n",
    "            sys.exit(0)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 0 2016-01-01 2019-01-01 {'index': 0, 'name': 'profit', 'lamb': 0, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AAPL 0 2016-01-01 2019-01-01 {'index': 1, 'name': 'sharpe', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AAPL 0 2016-01-01 2019-01-01 {'index': 2, 'name': 'volatility', 'lamb': 10, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AAPL 0 2016-01-01 2019-01-01 {'index': 3, 'name': 'regularized', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AAPL 0 2016-01-01 2019-01-01 {'index': 4, 'name': 'regularized', 'lamb': 0.05, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AAPL 0 2016-01-01 2019-01-01 {'index': 5, 'name': 'regularized', 'lamb': 0.1, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AAPL 0 2016-01-01 2019-01-01 {'index': 6, 'name': 'regularized', 'lamb': 0.2, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AMGN 0 2016-01-01 2019-01-01 {'index': 0, 'name': 'profit', 'lamb': 0, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AMGN 0 2016-01-01 2019-01-01 {'index': 1, 'name': 'sharpe', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AMGN 0 2016-01-01 2019-01-01 {'index': 2, 'name': 'volatility', 'lamb': 10, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AMGN 0 2016-01-01 2019-01-01 {'index': 3, 'name': 'regularized', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AMGN 0 2016-01-01 2019-01-01 {'index': 4, 'name': 'regularized', 'lamb': 0.05, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AMGN 0 2016-01-01 2019-01-01 {'index': 5, 'name': 'regularized', 'lamb': 0.1, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AMGN 0 2016-01-01 2019-01-01 {'index': 6, 'name': 'regularized', 'lamb': 0.2, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AXP 0 2016-01-01 2019-01-01 {'index': 0, 'name': 'profit', 'lamb': 0, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AXP 0 2016-01-01 2019-01-01 {'index': 1, 'name': 'sharpe', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AXP 0 2016-01-01 2019-01-01 {'index': 2, 'name': 'volatility', 'lamb': 10, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AXP 0 2016-01-01 2019-01-01 {'index': 3, 'name': 'regularized', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AXP 0 2016-01-01 2019-01-01 {'index': 4, 'name': 'regularized', 'lamb': 0.05, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AXP 0 2016-01-01 2019-01-01 {'index': 5, 'name': 'regularized', 'lamb': 0.1, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "AXP 0 2016-01-01 2019-01-01 {'index': 6, 'name': 'regularized', 'lamb': 0.2, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "BA 0 2016-01-01 2019-01-01 {'index': 0, 'name': 'profit', 'lamb': 0, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "BA 0 2016-01-01 2019-01-01 {'index': 1, 'name': 'sharpe', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "BA 0 2016-01-01 2019-01-01 {'index': 2, 'name': 'volatility', 'lamb': 10, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "BA 0 2016-01-01 2019-01-01 {'index': 3, 'name': 'regularized', 'lamb': 0.01, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "BA 0 2016-01-01 2019-01-01 {'index': 4, 'name': 'regularized', 'lamb': 0.05, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "BA 0 2016-01-01 2019-01-01 {'index': 5, 'name': 'regularized', 'lamb': 0.1, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "BA 0 2016-01-01 2019-01-01 {'index': 6, 'name': 'regularized', 'lamb': 0.2, 'sharpe_list': [], 'cumreturn_list': []}\n",
      "CAT 0 2016-01-01 2019-01-01 {'index': 0, 'name': 'profit', 'lamb': 0, 'sharpe_list': [], 'cumreturn_list': []}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    initial_investment = 1000\n",
    "    epochs = 10\n",
    "\n",
    "    kwargs = {\n",
    "        \"begin_date\": '2016-01-01', \n",
    "        \"end_date\": '2019-01-01', \n",
    "        \"split_point\": '2018-01-01', \n",
    "        \"load_from_file\": True, \n",
    "        \"transaction_cost\": 0.0000,\n",
    "        \"initial_investment\": initial_investment,\n",
    "        \"state_mode\": 1,\n",
    "        \"seed\": 42, \n",
    "        \"GAMMA\": 0.7, \n",
    "        \"n_step\": 5, \n",
    "        \"BATCH_SIZE\": 10, \n",
    "        \"ReplayMemorySize\": 20,\n",
    "        \"TARGET_UPDATE\": 5,\n",
    "        \"window_size\": None, \n",
    "        \"lamb\": 0.0,\n",
    "        \"n_episodes\": epochs,\n",
    "    }\n",
    "\n",
    "    import multiprocessing\n",
    "\n",
    "    NUM_PROCESSES = 2\n",
    "    NUM_QUEUE_ITEMS = 20\n",
    "\n",
    "    # queue = multiprocessing.Queue(maxsize=NUM_QUEUE_ITEMS)\n",
    "    # pool = multiprocessing.Pool(NUM_PROCESSES, worker_main, (queue,))\n",
    "    \n",
    "    _begin_date = '20{}-01-01'\n",
    "    _end_date = '20{}-01-01'\n",
    "    _split_point = '20{}-01-01' \n",
    "\n",
    "\n",
    "    files = os.listdir(\"./Data/\")\n",
    "    ls = []\n",
    "    for _file in files[:]:\n",
    "\n",
    "        for year in range(4):\n",
    "            begin_date = _begin_date.format(16+year)\n",
    "            end_date = _end_date.format(19+year)\n",
    "            split_point = _split_point.format(18+year)\n",
    "\n",
    "            for seed in range(100):\n",
    "\n",
    "                # model_files = os.listdir(\"./Results/AAPL/Train/\")\n",
    "                # for m_file in model_files: os.remove(f\"./Results/AAPL/Train/{m_file}\")\n",
    "\n",
    "                path = f\"./Results/{_file}/{begin_date}~{end_date}/{seed}/train_log/{seed}.log\"\n",
    "                is_finished = check_log(path, epochs)            \n",
    "                if is_finished: \n",
    "                    print(f\"symbol: {_file}, seed: {seed} 已经跑过了，跳过...\")\n",
    "                    continue\n",
    "                item = (_file, begin_date, end_date, split_point, initial_investment, seed, epochs)\n",
    "                run(*item)\n",
    "\n",
    "                # while True:\n",
    "                #     if not queue.full():\n",
    "                #         print(f\"加入seed: {seed}到队列...\")\n",
    "                #         queue.put(item)\n",
    "                #         break\n",
    "                #     else:\n",
    "                #         print(\"queue已满，休息10秒...\")\n",
    "                #         time.sleep(10)\n",
    "    \n",
    "    # queue.put(None)\n",
    "\n",
    "    # queue.close()\n",
    "    # queue.join_thread()\n",
    "\n",
    "    # pool.close()\n",
    "    # pool.join()\n",
    "\n",
    "    # import sys \n",
    "    # sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('finrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c5bd0f089c78e1f78e4e2358186f893a869a12cc23d478c7e1a14f969727697d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
