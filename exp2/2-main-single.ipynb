{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "from DataLoader.DataLoader import DataLoader\n",
    "from DataLoader.DataBasedAgent import DataBasedAgent\n",
    "from DataLoader.DataRLAgent import DataRLAgent\n",
    "import DeepRLAgent.VanillaInput.Train as Train\n",
    "from PatternDetectionInCandleStick.Evaluation import Evaluation\n",
    "import distinctipy\n",
    "\n",
    "import utils\n",
    "from importlib import reload\n",
    "import re\n",
    "from utils import setup_logger\n",
    "\n",
    "Train = reload(Train)\n",
    "DeepRL = Train.Train\n",
    "utils = reload(utils)\n",
    "from utils import add_train_portfo, add_test_portfo, plot_return, calc_return, plot_action_point, calc_bh\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "device = \"cpu\"\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CURRENT_PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    DATASET_NAME, \n",
    "    split_point='2018-01-01', \n",
    "    begin_date='2010-01-01', \n",
    "    end_date='2020-08-24', \n",
    "    initial_investment=1000,\n",
    "    transaction_cost=0.0001,\n",
    "    load_from_file=True,\n",
    "    reward_type=\"profit\",\n",
    "    seed=42, \n",
    "    state_mode=1,\n",
    "    n_episodes=5,\n",
    "    lamb=0.0001,\n",
    "    GAMMA=0.7, \n",
    "    n_step=5, \n",
    "    BATCH_SIZE=10, \n",
    "    ReplayMemorySize=20,\n",
    "    TARGET_UPDATE=5,\n",
    "    window_size=None, \n",
    "    train_portfolios={},\n",
    "    test_portfolios={},\n",
    "    arms={},\n",
    "    show_all = False,\n",
    "    ratio_threshold=0.9,\n",
    "):\n",
    "    data_loader = DataLoader(DATASET_NAME, split_point=split_point, begin_date=begin_date, end_date=end_date, load_from_file=load_from_file)\n",
    "    \n",
    "    dataTrain_agent = DataRLAgent(data_loader.data_train, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "    dataTest_agent = DataRLAgent(data_loader.data_test, state_mode, 'action_encoder_decoder', device, GAMMA, n_step, BATCH_SIZE, window_size, transaction_cost)\n",
    "    # NOTE 这俩是b&h\n",
    "    dataTrain_base = DataBasedAgent(data_loader.data_train, data_loader.patterns, 'action_deepRL', device, GAMMA, n_step, BATCH_SIZE, transaction_cost)\n",
    "    dataTest_base = DataBasedAgent(data_loader.data_test, data_loader.patterns, 'action_deepRL', device, GAMMA, n_step, BATCH_SIZE, transaction_cost)\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    agent = DeepRL(data_loader, dataTrain_agent, dataTest_agent, \n",
    "                DATASET_NAME,  state_mode, window_size, transaction_cost,\n",
    "                BATCH_SIZE=BATCH_SIZE, GAMMA=GAMMA, ReplayMemorySize=ReplayMemorySize,\n",
    "                TARGET_UPDATE=TARGET_UPDATE, n_step=n_step, arms=arms)\n",
    "\n",
    "    agent.train(arms, n_episodes, ratio_threshold, seed, begin_date, end_date)\n",
    "\n",
    "    agent_eval = agent.test(initial_investment=initial_investment, test_type='train', model_dir=\"\")\n",
    "    train_portfolio = agent_eval.get_daily_portfolio_value()\n",
    "    \n",
    "    agent_test = agent.test(initial_investment=initial_investment, test_type='test', model_dir=\"\")\n",
    "    test_portfolio = agent_test.get_daily_portfolio_value()\n",
    "\n",
    "    # 选出最优的arm    \n",
    "    max_index = int(sorted(arms, key=lambda x: x[\"theta\"], reverse=True)[0][\"index\"])\n",
    "    arm = arms[max_index]\n",
    "    final_reward_type = f\"{arm['name']}_{arm['lamb']}_{seed}\"\n",
    "\n",
    "    final_model_name = f'DQN-stock:{DATASET_NAME}-final_reward:{final_reward_type}-epochs:{n_episodes}-seed:{seed}'\n",
    "\n",
    "    add_train_portfo(train_portfolios, final_model_name, train_portfolio)\n",
    "    add_test_portfo(test_portfolios, final_model_name, test_portfolio)\n",
    "\n",
    "    if show_all:\n",
    "        path = f\"./Results/{DATASET_NAME}/{begin_date}~{end_date}/{seed}/train\"\n",
    "        dirs = os.listdir(path)\n",
    "        for _dir in dirs:\n",
    "            if f\"_{seed}.pkl\" not in _dir: continue\n",
    "            if _dir == f\"model_{seed}.pkl\": continue\n",
    "            \n",
    "            model_dir = f\"{path}/{_dir}\"\n",
    "            agent_eval = agent.test(initial_investment=initial_investment, test_type='train', model_dir=model_dir)\n",
    "            train_portfolio = agent_eval.get_daily_portfolio_value()\n",
    "            \n",
    "            agent_test = agent.test(initial_investment=initial_investment, test_type='test', model_dir=model_dir)\n",
    "            test_portfolio = agent_test.get_daily_portfolio_value()\n",
    "\n",
    "            reward_type = re.findall(\"model_(.*?).pkl\", _dir)[0]\n",
    "            model_name = f'DQN-stock:{DATASET_NAME}-reward:{reward_type}-epochs:{n_episodes}-seed:{seed}'\n",
    "\n",
    "            add_train_portfo(train_portfolios, model_name, train_portfolio)\n",
    "            add_test_portfo(test_portfolios, model_name, test_portfolio)\n",
    "\n",
    "    # plot_action_point(\n",
    "    #     \"test\", \n",
    "    #     dataTrain_agent, \n",
    "    #     dataTest_agent, \n",
    "    #     data_loader, \n",
    "    #     \"DQN\", \n",
    "    #     DATASET_NAME, \n",
    "    #     begin=0, end=100\n",
    "    # )\n",
    "    \n",
    "    calc_bh(train_portfolios, test_portfolios, data_loader, initial_investment)\n",
    "    indexes = calc_return(data_loader, train_portfolios, test_portfolios)\n",
    "    \n",
    "    flag_biggest = False\n",
    "    flag_top_3 = False\n",
    "    if indexes.T[\"sharpe_train\"][final_model_name] == indexes.T[\"sharpe_train\"].max():\n",
    "        flag_biggest = True\n",
    "    if indexes.T[\"sharpe_train\"][final_model_name] in indexes.T[\"sharpe_train\"].sort_values(ascending=False).values[:3]:\n",
    "        flag_top_3 = True\n",
    "    \n",
    "    path = f\"./Results/{DATASET_NAME}/{begin_date}~{end_date}/{seed}/train_log/\"\n",
    "    logger, handler = setup_logger(f'{DATASET_NAME}-{seed}-final', f'{path}/{seed}.log')\n",
    "    logger.info(f\"symbol: {DATASET_NAME}, seed: {seed}, final reward type: {indexes.T['sharpe_train'][final_model_name]}\")\n",
    "    logger.info(f\"symbol: {DATASET_NAME}, seed: {seed}, top 3: {indexes.T['sharpe_train'].sort_values(ascending=False).values[:3]}\")\n",
    "    logger.info(f\"result: biggest: {flag_biggest}, top 3: {flag_top_3}\")\n",
    "    logger.removeHandler(handler)\n",
    "    return indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_portfolios = {}\n",
    "test_portfolios = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AAPL 0\n",
      "AAPL 1\n",
      "AAPL 2\n",
      "AAPL 3\n",
      "AAPL 4\n",
      "AAPL 5\n",
      "AAPL 6\n",
      "AAPL 7\n",
      "AAPL 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zengz\\my\\projects\\dynamic_rewards\\try6_with_regularized_rewards\\2-main-single.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m add_arm(arms, \u001b[39m\"\u001b[39m\u001b[39mregularized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0.2\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m kwargs\u001b[39m.\u001b[39mupdate({\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mDATASET_NAME\u001b[39m\u001b[39m\"\u001b[39m: _file,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mreward_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtest_portfolios\u001b[39m\u001b[39m\"\u001b[39m: test_portfolios,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m })\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m indexes \u001b[39m=\u001b[39m train(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m ls\u001b[39m.\u001b[39mappend(indexes)\n",
      "\u001b[1;32mc:\\Users\\zengz\\my\\projects\\dynamic_rewards\\try6_with_regularized_rewards\\2-main-single.ipynb Cell 5\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(DATASET_NAME, split_point, begin_date, end_date, initial_investment, transaction_cost, load_from_file, reward_type, seed, state_mode, n_episodes, lamb, GAMMA, n_step, BATCH_SIZE, ReplayMemorySize, TARGET_UPDATE, window_size, train_portfolios, test_portfolios, arms, show_all, ratio_threshold)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m torch\u001b[39m.\u001b[39mmanual_seed(seed)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m agent \u001b[39m=\u001b[39m DeepRL(data_loader, dataTrain_agent, dataTest_agent, \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m             DATASET_NAME,  state_mode, window_size, transaction_cost,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m             BATCH_SIZE\u001b[39m=\u001b[39mBATCH_SIZE, GAMMA\u001b[39m=\u001b[39mGAMMA, ReplayMemorySize\u001b[39m=\u001b[39mReplayMemorySize,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m             TARGET_UPDATE\u001b[39m=\u001b[39mTARGET_UPDATE, n_step\u001b[39m=\u001b[39mn_step, arms\u001b[39m=\u001b[39marms)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m agent\u001b[39m.\u001b[39;49mtrain(arms, n_episodes, ratio_threshold, seed, begin_date, end_date)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m agent_eval \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39mtest(initial_investment\u001b[39m=\u001b[39minitial_investment, test_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, model_dir\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zengz/my/projects/dynamic_rewards/try6_with_regularized_rewards/2-main-single.ipynb#W3sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m train_portfolio \u001b[39m=\u001b[39m agent_eval\u001b[39m.\u001b[39mget_daily_portfolio_value()\n",
      "File \u001b[1;32mc:\\Users\\zengz\\my\\projects\\dynamic_rewards\\try6_with_regularized_rewards\\DeepRLAgent\\BaseTrain.py:226\u001b[0m, in \u001b[0;36mBaseTrain.train\u001b[1;34m(self, arms, num_episodes, ratio_threshold, seed, begin_date, end_date)\u001b[0m\n\u001b[0;32m    223\u001b[0m     state \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_train\u001b[39m.\u001b[39mget_current_state()], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    225\u001b[0m \u001b[39m# Perform one step of the optimization (on the target network)\u001b[39;00m\n\u001b[1;32m--> 226\u001b[0m _loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimize_model()\n\u001b[0;32m    227\u001b[0m loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m _loss\n\u001b[0;32m    229\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[1;32mc:\\Users\\zengz\\my\\projects\\dynamic_rewards\\try6_with_regularized_rewards\\DeepRLAgent\\BaseTrain.py:141\u001b[0m, in \u001b[0;36mBaseTrain.optimize_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_net\u001b[39m.\u001b[39mparameters():\n\u001b[0;32m    140\u001b[0m     param\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mclamp_(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[1;32m--> 141\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\zengz\\my\\anaconda\\envs\\finrl\\lib\\site-packages\\torch\\optim\\optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zengz\\my\\anaconda\\envs\\finrl\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\zengz\\my\\anaconda\\envs\\finrl\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\zengz\\my\\anaconda\\envs\\finrl\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\zengz\\my\\anaconda\\envs\\finrl\\lib\\site-packages\\torch\\optim\\adam.py:256\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[39massert\u001b[39;00m param\u001b[39m.\u001b[39mis_cuda \u001b[39mand\u001b[39;00m step_t\u001b[39m.\u001b[39mis_cuda, \u001b[39m\"\u001b[39m\u001b[39mIf capturable=True, params and state_steps must be CUDA tensors.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    255\u001b[0m \u001b[39m# update step\u001b[39;00m\n\u001b[1;32m--> 256\u001b[0m step_t \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    258\u001b[0m \u001b[39mif\u001b[39;00m weight_decay \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    259\u001b[0m     grad \u001b[39m=\u001b[39m grad\u001b[39m.\u001b[39madd(param, alpha\u001b[39m=\u001b[39mweight_decay)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATASET_NAME = r'AAPL'\n",
    "\n",
    "initial_investment = 1000\n",
    "\n",
    "kwargs = {\n",
    "    \"DATASET_NAME\": DATASET_NAME, \n",
    "    \"begin_date\": '2016-01-01', \n",
    "    \"end_date\": '2019-01-01', \n",
    "    \"split_point\": '2018-01-01', \n",
    "    \"load_from_file\": True, \n",
    "    \"transaction_cost\": 0.0000,\n",
    "    \"initial_investment\": initial_investment,\n",
    "    \"state_mode\": 1,\n",
    "    \"seed\": 42, \n",
    "    \"GAMMA\": 0.7, \n",
    "    \"n_step\": 5, \n",
    "    \"BATCH_SIZE\": 10, \n",
    "    \"ReplayMemorySize\": 20,\n",
    "    \"TARGET_UPDATE\": 5,\n",
    "    \"window_size\": None, \n",
    "    \"train_portfolios\": train_portfolios,\n",
    "    \"test_portfolios\": test_portfolios,\n",
    "    \"lamb\": 0.0,\n",
    "}\n",
    "\n",
    "# NOTE reward_types: profit, sharpe, volatility, regularized\n",
    "# NOTE sharpe: lamb:0.01；volatility: lamb: 10\n",
    "\n",
    "def add_arm(arms, name, lamb):\n",
    "    arm = { \"index\": len(arms), \"name\": name, \"theta\": 0, \"a\": 1, \"b\": 1, \"sharpe_list\": [], \"cumreturn_list\": [], \"lamb\": lamb, \"used\": 0},\n",
    "    arms.extend(arm)\n",
    "\n",
    "\n",
    "files = os.listdir(\"./Data/\")\n",
    "ls = []\n",
    "for _file in files[:1]:\n",
    "    # NOTE: seed相当于是simulation\n",
    "    for seed in range(20):\n",
    "        print(_file, seed)\n",
    "        \n",
    "        train_portfolios = {}\n",
    "        test_portfolios = {}\n",
    "\n",
    "        # model_files = os.listdir(\"./Results/AAPL/Train/\")\n",
    "        # for m_file in model_files: os.remove(f\"./Results/AAPL/Train/{m_file}\")\n",
    "\n",
    "        arms = []\n",
    "        add_arm(arms, \"old_profit\", 0)\n",
    "        add_arm(arms, \"future_profit_1\", 0)\n",
    "        add_arm(arms, \"future_profit_10\", 0)\n",
    "\n",
    "        kwargs.update({\n",
    "            \"DATASET_NAME\": _file,\n",
    "            \"reward_type\": \"\",\n",
    "            \"seed\": seed,\n",
    "            \"n_episodes\": 20,\n",
    "            \"arms\": arms,\n",
    "            \"show_all\": True,\n",
    "            \"ratio_threshold\": 3,\n",
    "            \"train_portfolios\": train_portfolios,\n",
    "            \"test_portfolios\": test_portfolios,\n",
    "        })\n",
    "        \n",
    "        indexes = train(**kwargs)\n",
    "        ls.append(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "a = []\n",
    "for _file in files:\n",
    "    symbol = _file.replace(\".csv\", \"\")\n",
    "    keys = [key for key in test_portfolios.keys() if symbol in key]\n",
    "    \n",
    "    ls = []\n",
    "    ls2 = []\n",
    "    for key in keys:\n",
    "        profit_percentage = pd.DataFrame(test_portfolios[key]).pct_change(1)\n",
    "        total_return = test_portfolios[key][-1] / test_portfolios[key][0] - 1 \n",
    "        if profit_percentage.std()[0] > 0:\n",
    "            sharpe = np.sqrt(252) * profit_percentage.mean()[0] / profit_percentage.std()[0]\n",
    "        else:\n",
    "            sharpe = 0\n",
    "        ls.append(sharpe)\n",
    "        ls2.append(total_return)\n",
    "    if len(ls) == 0: continue\n",
    "    df = pd.read_csv(f\"./Data/{_file}/{_file}.csv\")\n",
    "    df = df.iloc[2013:]\n",
    "    pct = df[\"Close\"].pct_change(1)\n",
    "    bh_return = df[\"Close\"].iloc[-1] / df[\"Close\"].iloc[0] - 1\n",
    "    bh_sharpe = np.sqrt(252) * pct.mean() / pct.std()\n",
    "    a.append(symbol)\n",
    "    print(f\"stock: {symbol}, sharpe: {round(np.median(ls), 4)}, bh_sharpe: {round(bh_sharpe, 4)}, total return: {round(np.median(ls2), 4)}, bh_return: {round(bh_return, 4)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colors = distinctipy.get_colors(len(train_portfolios.items()))\n",
    "# plot_return(\"train\", DATASET_NAME, data_loader, train_portfolios, test_portfolios, colors, indexes)\n",
    "# plot_return(\"test\", DATASET_NAME, data_loader, train_portfolios, test_portfolios, colors, indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indexes.loc[(indexes.index != 'mdd_date_train') & (indexes.index != 'mdd_date_test')].mean(axis=1).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c5bd0f089c78e1f78e4e2358186f893a869a12cc23d478c7e1a14f969727697d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
